\section{Research Experience}
\begin{itemize}
      \item \heading{\href{https://ruqizhang.github.io/}{RZ-Lab}, Purdue University}{USA}
            {\emph{Research Intern,} Advised by \href{https://ruqizhang.github.io/}{\textbf{Prof. Ruqi Zhang}}}{May 2024--Present}
            \begin{itemize}
                  \item \detail{}{Proposed a two-phase plug-and-play alignment framework (ETA) featuring a multimodal evaluator and bi-level alignment, provided a new perspective of safety challenges in vision-language models caused by continuous visual token embeddings. ETA ensures responses are both safe and useful, improving harmlessness and helpfulness without additional training or data while maintaining the VLMâ€™s general performance. \emph{(Accepted at ICLR 2025)}}{}
            \end{itemize}

        \item \heading{\href{https://open-trust-lab.vercel.app/}{Open Trust Lab}, Shanghai AI Laboratory}{Beijing, China}
            {\emph{Research Intern,} Advised by \href{https://openreview.net/profile?id=~Lijun_Li1}{\textbf{Dr. Lijun Li}}}{Dec. 2024--Present}
            \begin{itemize}
                  \item \detail{}{Proposed a new safety fine-tuning paradigm by constructing multi-image unsafe scenarios, creating SFT data with visual reasoning, while improving the model's helpfulness and harmlessness. Additionally, I introduced a large-scale multi-image safety benchmark, which demonstrates how current models struggle to capture unsafe intentions formed by associations between images, leading to successful jailbreaks. \emph{(In Submission)}}{}
            \end{itemize}
        
        \item \heading{\href{https://tj.teacher.360eol.com/teacherBasic/preview?teacherId=10852}{MLDM Lab}, Tianjin University}{Tianjin, China}
            {\emph{Research Intern,} Advised by \href{https://bcaosudo.github.io/}{\textbf{Prof. Bing Cao}} and \href{https://cic.tju.edu.cn/faculty/huqinghua/index.html}{\textbf{Prof. Qinghua Hu}}}{Sep. 2023--Dec. 2024}
            \begin{itemize}
                \item \detail{}{Revealed that the key to dynamic fusion lies in the correlation between the weights and the loss, providing theoretical foundations for multimodal decision-level fusion from the perspective of the generalization error bound. Based on this insight, proposed the \emph{Predictive Dynamic Fusion} (PDF) algorithmic framework offers trustworthy priors for decision-level fusion in multimodal systems or multi-agent settings, thereby achieving better generalization. \emph{(Accepted at ICML 2024)}}{}
                % \item \detail{}{Based on Predictive Dynamic Fusion, revealing that the relative scale of predictions is enough for fusion. Introduced a modality-level ranking loss, which enhances single-modal generalization and robustness during optimization}{}
                \item \detail{}{Proved theoretically that dynamic image fusion outperforms static image fusion and introduced Relative Dominability, providing a formal framework to enhance the interpretability of complex network architectures. This theoretical proof supports dynamically fusing advantageous regions and adjusting fusion weights at test time, leading to significant improvements in image quality across various baselines. \emph{(Accepted at NeurIPS 2024)}}{}
            \end{itemize}
            \item \heading{\href{https://tianlong-chen.github.io/}{UNITES Lab}, UNC Chapel Hill}{North Carolina, USA}{\emph{Research Intern}, \text{Advised by \href{https://tianlong-chen.github.io/}{\textbf{Prof. Tianlong Chen}}}}{Feb. 2024-May 2024}
            \begin{itemize}
                \item \detail{}{Developed a neural network ensemble framework for time series stock prediction, integrating multiple models \emph{(1D-CNN, GRU, etc.)} to improve accuracy. Additionally, proposed an efficient fine-tuning system for time series foundation models (EFT-TSFM) to enhance parameter and memory efficiency. Findings were summarized in the NN in Finance technical report.}{}
            \end{itemize}
\end{itemize}